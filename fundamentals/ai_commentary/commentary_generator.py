"""
AI Commentary Generator.
Uses Google Gemini API to generate investment analysis reports.
"""

import json
import logging
import requests
import time
from config.settings import settings
from typing import Dict, Optional, List, Any

# Setup logger with secure formatting (already in utils.logger)
# But we can just use print or standard logging if imported
from utils.logger import setup_logger

logger = setup_logger('ai_commentary')

class CommentaryGenerator:
    """Generates AI commentary using Google Gemini."""
    
    def __init__(self):
        self.api_key = settings.GOOGLE_AI_KEY
        if not self.api_key:
            logger.warning("Google AI Key not found. AI commentary will be disabled.")
            
        self.models_to_try = [
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite"
        ]

    def generate_report(self, aggregated_data: Dict[str, Any]) -> Optional[str]:
        """
        Generate markdown report from aggregated data.
        """
        if not self.api_key:
            return None
            
        prompt = self._build_prompt(aggregated_data)
        
        for model_name in self.models_to_try:
            try:
                print(f"   [AI] Attempting model: {model_name}...")
                logger.info(f"Attempting valid model: {model_name}")
                response = self._call_api(model_name, prompt)
                if response:
                    return response
            except Exception as e:
                logger.warning(f"Model {model_name} failed: {e}")
                continue
                
        return None

    def _build_prompt(self, data: Dict[str, Any]) -> str:
        """Construct the prompt from the template."""
        # Optimize: Remove indentation to save tokens
        json_str = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        
        # Helper to get max score safely
        fin = data.get('financial_score', {})
        prof = fin.get('profitability', {})
        growth = fin.get('growth', {})
        cap = fin.get('capital', {})
        
        def g(d, k): 
            val = d.get(k, {}).get('max', '-')
            # If value is 0 (disabled), keep it as 0 to indicate not used
            return val
            
        return f"""
<stock_data>
{json_str}
</stock_data>

åŸºäºæ•°æ®ç”ŸæˆMarkdownåˆ†ææŠ¥å‘Šã€‚
**æ ¸å¿ƒæŒ‡ä»¤ï¼š**
1. å…¨æ–‡æ‰€æœ‰"X"å‡éœ€ç”¨`<stock_data>`çœŸå®æ•°æ®æ›¿æ¢ï¼Œæ— æ•°æ®å¡«"-"ã€‚
2. éœ€è¦è€ƒè™‘åˆ†æå…¬å¸æ‰€åœ¨çš„è¡Œä¸šï¼Œä¸åŒè¡Œä¸šå„æŒ‡æ ‡çš„é‡è¦æ€§ä¸ä¸€ï¼Œç‰¹åˆ«æ˜¯ä¼°å€¼æ¨¡å‹ã€‚
3. ç»“æ„ä¸¥è°¨ï¼Œæ— ä»£ç å—ï¼Œè¨€ç®€æ„èµ…ã€‚

# ğŸ“Š X åˆ†ææŠ¥å‘Š (X)
**è¡Œä¸š:** X | **ä»·æ ¼:** $X

## ä¸€ã€è´¢åŠ¡åŸºæœ¬é¢ (å¾—åˆ†:X)
**è¯„:** [50å­—æ€»è¯„]

### 1. ç›ˆåˆ©èƒ½åŠ› (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | å¾—åˆ† | è¯„ |
|------|------|------|----|
| ROIC | X% | X/{g(prof, 'roic')} | [è¯„] |
| ROE | X% | X/{g(prof, 'roe')} | [è¯„] |
| è¥ä¸šåˆ©æ¶¦ç‡ | X% | X/{g(prof, 'op_margin')} | [è¯„] |
| æ¯›åˆ©ç‡ | X% | X/{g(prof, 'gross_margin')} | [è¯„] |
| å‡€åˆ©ç‡ | X% | X/{g(prof, 'net_margin')} | [è¯„] |

### 2. æˆé•¿æ€§ (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | å¾—åˆ† | è¯„ |
|------|------|------|----|
| FCFå¢é€Ÿ(5å¹´) | X% | X/{g(growth, 'fcf_cagr')} | [è¯„] |
| å‡€åˆ©å¢é€Ÿ(5å¹´) | X% | X/{g(growth, 'ni_cagr')} | [è¯„] |
| è¥æ”¶å¢é€Ÿ(5å¹´) | X% | X/{g(growth, 'rev_cagr')} | [è¯„] |
| ç›ˆåˆ©è´¨é‡ | X | X/{g(growth, 'quality')} | [è¯„] |
| FCF/å€ºåŠ¡ | X | X/{g(growth, 'debt')} | [è¯„] |

### 3. èµ„æœ¬é…ç½® (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | å¾—åˆ† | è¯„ |
|------|------|------|----|
| å›è´­æ”¶ç›Šç‡ | X% | X/{g(cap, 'buyback')} | [è¯„] |
| èµ„æœ¬æ”¯å‡º | X | X/{g(cap, 'capex')} | [è¯„] |
| è‚¡æƒæ¿€åŠ± | X | X/{g(cap, 'sbc')} | [è¯„] |

## äºŒã€æŠ€æœ¯é¢ (å¾—åˆ†:X)
**è¯„:** [50å­—æ€»è¯„]

### 1. è¶‹åŠ¿ä¸åŠ¨é‡ (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | ä¿¡å· | è¯„ |
|------|------|------|----|
| ADX | X | [ä¿¡å·] | [è¯„] |
| 52å‘¨ä½ç½® | X% | [ä¿¡å·] | [è¯„] |
| RSI | X | [ä¿¡å·] | [è¯„] |
| MACD | X | [ä¿¡å·] | [è¯„] |

### 2. æ³¢åŠ¨ä¸ç»“æ„ (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | ä¿¡å· | è¯„ |
|------|------|------|----|
| ATR | X% | [ä¿¡å·] | [è¯„] |
| å¸ƒæ—å¸¦ | - | [ä¿¡å·] | [è¯„] |
| æ”¯æ’‘/é˜»åŠ› | - | [ä¿¡å·] | [è¯„] |

## ä¸‰ã€ä¼°å€¼åˆ†æ (å…¬å…ä»·:$X | ä¸Šè¡Œç©ºé—´:X%)
**å½“å‰ä»·:** $X

| æ¨¡å‹ | å…¬å…ä»· | æƒé‡ | åç¦»åº¦ | è¯„ |
|------|--------|------|--------|----|
| PEä¼°å€¼ | $X | X% | X% | [è¯„] |
| PSä¼°å€¼ | $X | X% | X% | [è¯„] |
| PBä¼°å€¼ | $X | X% | X% | [è¯„] |
| EV/EBITDA| $X | X% | X% | [è¯„] |
| PEGä¼°å€¼ | $X | X% | X% | [è¯„] |
| DDMæ¨¡å‹ | $X | X% | X% | [è¯„] |
| DCFæ¨¡å‹ | $X | X% | X% | [è¯„] |
| åˆ†æå¸ˆç›®æ ‡| $X | X% | X% | [è¯„] |

## å››ã€æ€»ç»“ä¸å»ºè®®
**æ ¸å¿ƒä¼˜åŠ¿:** [3ç‚¹çŸ­è¯­]
**ä¸»è¦é£é™©:** [3ç‚¹çŸ­è¯­]
**ç»¼åˆç»“è®º:** [çº¦200å­—é€»è¾‘]

> **X æ“ä½œ:** [ä¹°å…¥|æŒæœ‰|è§‚æœ›|å–å‡º]
**ç†ç”±:** [100å­—å†…]
"""

    def _call_api(self, model_name: str, prompt: str) -> Optional[str]:
        """Call Gemini API with retry logic."""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={self.api_key}"
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 16384  # Increased for complete reports
            }
        }
        
        max_retries = 1
        for attempt in range(max_retries):
            try:
                response = requests.post(url, json=payload, timeout=60)
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # Extract usage metadata
                    usage = result.get("usageMetadata", {})
                    total_tokens = usage.get("totalTokenCount", 0)
                    print(f"   [AI] Success! Model: {model_name} | Tokens Used: {total_tokens}")
                    
                    candidates = result.get("candidates", [])
                    if candidates:
                        candidate = candidates[0]
                        finish_reason = candidate.get("finishReason", "")
                        
                        # Warn if truncated
                        if finish_reason == "MAX_TOKENS":
                            logger.warning(f"Response truncated (MAX_TOKENS). Consider increasing limit.")
                            print(f"   [WARN] Response may be incomplete (hit token limit)")
                        
                        return candidate.get("content", {}).get("parts", [])[0].get("text", "")
                    return None # Empty response
                
                # Handle Rate Limits (429)
                if response.status_code == 429:
                    wait_time = 5 * (attempt + 1)
                    print(f"   [AI] Rate limited on {model_name}. Retrying in {wait_time}s...")
                    logger.warning(f"Rate limited (429) on {model_name}. Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                
                # Handle 404
                if response.status_code == 404:
                    print(f"   [AI] Model {model_name} not found.")
                    logger.warning(f"Model {model_name} not found (404).")
                    return None
                    
                logger.warning(f"API Error {model_name} ({response.status_code}): {response.text}")
                return None
                
            except Exception as e:
                logger.warning(f"Exception calling {model_name}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
        return None
