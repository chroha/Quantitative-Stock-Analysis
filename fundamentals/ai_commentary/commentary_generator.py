"""
AI Commentary Generator.
Uses Google Gemini API to generate investment analysis reports.
"""

import json
import logging
import requests
import time
from config.settings import settings
from typing import Dict, Optional, List, Any

# Setup logger with secure formatting (already in utils.logger)
# But we can just use print or standard logging if imported
from utils.logger import setup_logger

logger = setup_logger('ai_commentary')

class CommentaryGenerator:
    """Generates AI commentary using Google Gemini."""
    
    def __init__(self):
        self.api_key = settings.GOOGLE_AI_KEY
        if not self.api_key:
            logger.warning("Google AI Key not found. AI commentary will be disabled.")
            
        self.models_to_try = [
            "gemini-2.5-pro",
            "gemini-2.5-flash",
            "gemini-2.5-flash-lite"
        ]

    def generate_report(self, aggregated_data: Dict[str, Any]) -> Optional[str]:
        """
        Generate markdown report from aggregated data.
        """
        if not self.api_key:
            return None
            
        prompt = self._build_prompt(aggregated_data)
        
        for model_name in self.models_to_try:
            try:
                print(f"   [AI] Attempting model: {model_name}...")
                logger.info(f"Attempting valid model: {model_name}")
                response = self._call_api(model_name, prompt)
                if response:
                    return response
            except Exception as e:
                logger.warning(f"Model {model_name} failed: {e}")
                continue
                
        return None

    def _build_prompt(self, data: Dict[str, Any]) -> str:
        """Construct the prompt from the template."""
        # Optimize: Remove indentation to save tokens
        json_str = json.dumps(data, ensure_ascii=False, separators=(',', ':'))
        
        # Helper to get max score safely
        fin = data.get('financial_score', {})
        prof = fin.get('profitability', {})
        growth = fin.get('growth', {})
        cap = fin.get('capital', {})
        
        def g(d, k): 
            val = d.get(k, {}).get('max', '-')
            # If value is 0 (disabled), keep it as 0 to indicate not used
            return val
            
        stock_info = data.get('stock_info', {})
        latest_period = stock_info.get('latest_period', 'Unknown')
        history_years = stock_info.get('history_years', '?')
            
        return f"""
<stock_data>
{json_str}
</stock_data>

åŸºäºæ•°æ®ç”ŸæˆMarkdownåˆ†ææŠ¥å‘Šã€‚
**æ ¸å¿ƒæŒ‡ä»¤ï¼š**
1. å…¨æ–‡æ‰€æœ‰"X"å‡éœ€ç”¨`<stock_data>`çœŸå®æ•°æ®æ›¿æ¢ï¼Œæ— æ•°æ®å¡«"-"ã€‚
2. éœ€è¦è€ƒè™‘åˆ†æå…¬å¸æ‰€åœ¨çš„è¡Œä¸šï¼Œä¸åŒè¡Œä¸šå„æŒ‡æ ‡çš„é‡è¦æ€§ä¸ä¸€ï¼Œç‰¹åˆ«æ˜¯ä¼°å€¼æ¨¡å‹ã€‚
3. ç»“æ„ä¸¥è°¨ï¼Œæ— ä»£ç å—ï¼Œè¨€ç®€æ„èµ…ã€‚

# ğŸ“Š X åˆ†ææŠ¥å‘Š (X)
**è¡Œä¸š:** X | **ä»·æ ¼:** $X
> **æ•°æ®æ¥æº:** åŸºäºæœ€æ–°è‡³ {latest_period} è´¢æŠ¥æ•°æ®ï¼Œæ¶µç›–è¿‡å» {history_years} å¹´è´¢åŠ¡å†å²ã€‚

## ä¸€ã€è´¢åŠ¡åŸºæœ¬é¢ (å¾—åˆ†:X)
**è¯„:** [æ€»è¯„]

### 1. ç›ˆåˆ©èƒ½åŠ› (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | å¾—åˆ† | è§£è¯» |
|------|------|------|----|
| ROIC | X% | X/{g(prof, 'roic')} | [è§£è¯»] |
| ROE | X% | X/{g(prof, 'roe')} | [è§£è¯»] |
| è¥ä¸šåˆ©æ¶¦ç‡ | X% | X/{g(prof, 'op_margin')} | [è§£è¯»] |
| æ¯›åˆ©ç‡ | X% | X/{g(prof, 'gross_margin')} | [è§£è¯»] |
| å‡€åˆ©ç‡ | X% | X/{g(prof, 'net_margin')} | [è§£è¯»] |

### 2. æˆé•¿æ€§ (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | å¾—åˆ† | è§£è¯» |
|------|------|------|----|
| FCFå¢é€Ÿ(5å¹´) | X% | X/{g(growth, 'fcf_cagr')} | [è§£è¯»] |
| å‡€åˆ©å¢é€Ÿ(5å¹´) | X% | X/{g(growth, 'ni_cagr')} | [è§£è¯»] |
| è¥æ”¶å¢é€Ÿ(5å¹´) | X% | X/{g(growth, 'rev_cagr')} | [è§£è¯»] |
| ç›ˆåˆ©è´¨é‡ | X | X/{g(growth, 'quality')} | [è§£è¯»] |
| FCF/å€ºåŠ¡ | X | X/{g(growth, 'debt')} | [è§£è¯»] |

### 3. èµ„æœ¬é…ç½® (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | å¾—åˆ† | è§£è¯» |
|------|------|------|----|
| å›è´­æ”¶ç›Šç‡ | X% | X/{g(cap, 'buyback')} | [è§£è¯»] |
| èµ„æœ¬æ”¯å‡º | X | X/{g(cap, 'capex')} | [è§£è¯»] |
| è‚¡æƒæ¿€åŠ± | X | X/{g(cap, 'sbc')} | [è§£è¯»] |

## äºŒã€æŠ€æœ¯é¢ (å¾—åˆ†:X)
**è¯„:** [æ€»è¯„]

### 1. è¶‹åŠ¿ä¸åŠ¨é‡ (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | ä¿¡å· | è§£è¯» |
|------|------|------|----|
| ADX | X | [ä¿¡å·] | [è§£è¯»] |
| 52å‘¨ä½ç½® | X% | [ä¿¡å·] | [è§£è¯»] |
| RSI | X | [ä¿¡å·] | [è§£è¯»] |
| MACD | X | [ä¿¡å·] | [è§£è¯»] |

### 2. æ³¢åŠ¨ä¸ç»“æ„ (X/X)
| æŒ‡æ ‡ | æ•°å€¼ | ä¿¡å· | è§£è¯» |
|------|------|------|----|
| ATR | X% | [ä¿¡å·] | [è§£è¯»] |
| å¸ƒæ—å¸¦ | - | [ä¿¡å·] | [è§£è¯»] |
| æ”¯æ’‘/é˜»åŠ› | - | [ä¿¡å·] | [è§£è¯»] |

## ä¸‰ã€ä¼°å€¼åˆ†æ (åŠ æƒä¼°ä»·:$X)
**å½“å‰ä»·:** $X | ä¸Šè¡Œç©ºé—´:X%

| æ¨¡å‹ | å…¬å…ä»· | æƒé‡ | åç¦»åº¦ | è§£è¯» |
|------|--------|------|--------|----|
| PEä¼°å€¼ | $X | X% | X% | [è§£è¯»] |
| PSä¼°å€¼ | $X | X% | X% | [è§£è¯»] |
| PBä¼°å€¼ | $X | X% | X% | [è§£è¯»] |
| EV/EBITDA| $X | X% | X% | [è§£è¯»] |
| PEGä¼°å€¼ | $X | X% | X% | [è§£è¯»] |
| DDMæ¨¡å‹ | $X | X% | X% | [è§£è¯»] |
| DCFæ¨¡å‹ | $X | X% | X% | [è§£è¯»] |
| åˆ†æå¸ˆç›®æ ‡| $X | X% | X% | [è§£è¯»] |

## å››ã€æ€»ç»“ä¸å»ºè®®
**æ ¸å¿ƒä¼˜åŠ¿:** [è¦ç‚¹åˆ†æ]
**ä¸»è¦é£é™©:** [è¦ç‚¹åˆ†æ]
**ç»¼åˆç»“è®º:** [çº¦200å­—é€»è¾‘]

> **X æ“ä½œ:** [ä¹°å…¥|æŒæœ‰|è§‚æœ›|å–å‡º]
**ç†ç”±:** [çº¦100å­—åˆ†æ]
"""

    def _call_api(self, model_name: str, prompt: str) -> Optional[str]:
        """Call Gemini API with retry logic."""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model_name}:generateContent?key={self.api_key}"
        
        payload = {
            "contents": [{
                "parts": [{"text": prompt}]
            }],
            "generationConfig": {
                "temperature": 0.7,
                # Maximizing output tokens for Gemini 1.5 Series (often caps at 8192, but 1.5 Pro/Flash can do more)
                "maxOutputTokens": 65536 
            }
        }
        
        max_retries = 1
        for attempt in range(max_retries):
            try:
                # Keep timeout at 120s to allow for long generations
                response = requests.post(url, json=payload, timeout=120)
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # Extract usage metadata
                    usage = result.get("usageMetadata", {})
                    total_tokens = usage.get("totalTokenCount", 0)
                    print(f"   [AI] Success! Model: {model_name} | Tokens Used: {total_tokens}")
                    
                    candidates = result.get("candidates", [])
                    if candidates:
                        candidate = candidates[0]
                        finish_reason = candidate.get("finishReason", "")
                        
                        # Warn if truncated
                        if finish_reason == "MAX_TOKENS":
                            logger.warning(f"Response truncated (MAX_TOKENS). Consider increasing limit.")
                            print(f"   [WARN] Response may be incomplete (hit token limit)")
                        
                        return candidate.get("content", {}).get("parts", [])[0].get("text", "")
                    return None # Empty response
                
                # Handle Rate Limits (429) or Server Overload (503)
                if response.status_code in [429, 503]:
                    code_msg = "Rate limit" if response.status_code == 429 else "Server overloaded"
                    wait_time = 5 * (attempt + 1)
                    print(f"   [AI] {code_msg} ({response.status_code}) on {model_name}. Retrying in {wait_time}s...")
                    logger.warning(f"{code_msg} (429/503) on {model_name}. Retrying in {wait_time}s...")
                    time.sleep(wait_time)
                    continue
                
                # Handle 404
                if response.status_code == 404:
                    print(f"   [AI] Model {model_name} not found.")
                    logger.warning(f"Model {model_name} not found (404).")
                    return None
                    
                logger.warning(f"API Error {model_name} ({response.status_code}): {response.text}")
                return None
                
            except Exception as e:
                logger.warning(f"Exception calling {model_name}: {e}")
                
                # If specific timeout error, log it clearly
                if "timed out" in str(e).lower():
                     print(f"   [AI] Request timed out (took >120s). Retrying...")
                
                if attempt < max_retries - 1:
                    time.sleep(5) # Standard wait for network errors
                    continue
        return None
